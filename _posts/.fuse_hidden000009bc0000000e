---
layout: post
comments: true
title:  "Linear Regression"
date:   2016-12-28 15:22:00
mathjax: true
categories: [supervised learning, regression, linear]
---

## Bài viết đang trong giai đoạn xây dựng, sẽ được hoàn thành sớm. 

<div class="imgcap">
<div >
<a href = "/2016/12/28/linearregression/">
    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png" width = "500"></a>
    <!-- <img src="/assets/rl/mdp.png" height="206"> -->
</div>
<div class="thecap"> Linear Regression <br> (Nguồn: <a href ="https://en.wikipedia.org/wiki/Linear_regression">Wikipedia</a>)</div>
</div>

Trong bài này, tôi sẽ giới thiều một trong những thuật toán cơ bản nhất (và đơn giản nhất) của Machine Learning. Đây là một thuật toán _Supervised learning_ có tên **Linear Regression** (Hồi Quy Tuyến Tính).

Trong trang này:

<!-- MarkdownTOC -->

- [1. Giới thiệu](#1-giới-thiệu)
- [2. Phân tích toán học](#2-phân-tích-toán-học)
    - [Dạng của Linear Regression](#dạng-của-linear-regression)
    - [Sai số dự đoán](#sai-số-dự-đoán)
    - [Hàm mất mát](#hàm-mất-mát)
    - [Nghiệm cho bài toán Linear Regression](#nghiệm-cho-bài-toán-linear-regression)
- [3. Triển khai trên trên Python](#3-triển-khai-trên-trên-python)
- [4. Thảo luận](#4-thảo-luận)
    - [Output là một vector nhiều biến](#output-là-một-vector-nhiều-biến)
    - [Mô hình là một đa thức bậc cao](#mô-hình-là-một-đa-thức-bậc-cao)
    - [Hạn chế của Linear Regression](#hạn-chế-của-linear-regression)
    - [Các phương pháp tối ưu](#các-phương-pháp-tối-ưu)
- [Tiep Vu](#tiep-vu)
- [5. Tài liệu tham khảo](#5-tài-liệu-tham-khảo)

<!-- /MarkdownTOC -->



## 1. Giới thiệu

Quay lại [ví dụ đơn giản được nêu trong bài trước](/2016/12/27/categories/): một căn nhà rộng \\(x_1 ~ \text{m}^2\\), có \\(x_2\\) phòng ngủ và cách trung tâm thành phố \\(x_3~ \text{km}\\) có giá là bao nhiêu. Giả sử chúng ta đã có số liệu thống kê từ 1000 căn nhà trong thành phố đó, liệu rằng khi có một căn nhà mới với các thông số về diện tích, số phòng ngủ và khoảng cách tới trung tâm, chúng ta có thể dự đoán được giá của căn phòng đó không? Nếu có thì hàm dự đoán \\(y = f(\mathbf{x}) \\) sẽ có dạng như thế nào. Ở đây \\(\mathbf{x} = [x_1; x_2; x_3] \\) là một vector
cột chứa thông tin _input_, \\(y\\) là một số vô hướng (scalar) biểu diễn _output_ (tức giá của căn nhà trong ví dụ này).

**Lưu ý về ký hiệu toán học:** _trong các bài viết của tôi, các số vô hướng được biểu diễn bởi các chữ cái viết ở dạng không in đậm, có thể viết hoa, ví dụ \\(x_1, N, y, k\\). Các vector được biểu diễn bằng các chữ cái thường in đậm, ví dụ \\(\mathbf{y}, \mathbf{x}_1 \\). Các ma trận được biểu diễn bởi các chữ viết hoa in đậm, ví dụ \\(\mathbf{X, Y, W} \\)._

Một cách đơn giản nhất, chúng ta có thể thấy rằng: i) diện tích nhà càng lớn thì giá nhà càng cao; ii) số lượng phòng ngủ càng lớn thì giá nhà càng cao; iii) càng xa trung tâm thì giá nhà càng giảm. Một hàm số đơn giản nhất có thể mô tả mối quan hệ giữa giá nhà và 3 đại lượng đầu vào là: 

\\[ \text{ giá nhà } \approx w_1 (\text{diện tích}) + w_2 (\text{số phòng}) + w_3 (\text{ khoảng cách}) + w_0 \\] 

trong đó, \\(w_1, w_2\\) là các hằng số dương, \\(w_3\\) là một hằng số âm, \\(w_0\\) là một hằng số được gọi là bias. Hay nói cách khác: 

\\[y \approx w_1 x_1 + w_2 x_2 + w_3 x_3 + w_0 = f(\mathbf{x}) = \bar{y}~~~~ (1)\\]

Mối quan hệ \\(y \approx f(\mathbf{x})\\) bên trên là một mối quan hệ tuyến tính (linear). Bài toán chúng ta đang làm là một bài toán thuộc loại regression. Bài toán đi tìm các hệ số tối ưu \\( \\{w_1, w_2, w_3, w_0 \\}\\) chính vì vậy được gọi là bài toán Linear Regression. 

**Chú ý 1:** \\(y\\) là giá trị thực của _outcome_ (dựa trên số liệu thống kê chúng ta có trong tập _training data_), trong khi \\(\bar{y}\\) là giá trị mà mô hình Linear Regression dự đoán được. Nhìn chung, \\(y\\) và \\(\bar{y}\\) là hai giá trị khác nhau do có sai số mô hình, tuy nhiên, chúng ta mong muốn rằng sự khác nhau này rất nhỏ.

**Chú ý 2:** _Linear_ hay _tuyến tính_ hiểu một cách đơn giản là _thẳng, phẳng_. Trong không gian hai chiều, một hàm số được gọi là _tuyến tính_ nếu đồ thị của nó có dạng một _đường thẳng_. Trong không gian ba chiều, một hàm số được goi là _tuyến tính_ nếu đồ thị của nó có dạng một _mặt phẳng_. Trong không gian nhiều hơn 3 chiều, khái niệm _mặt phẳng_ không còn phù hợp nữa, thay vào đó, một khái niệm khác ra đời được gọi là _siêu mặt phẳng_ (_hyperplane_). Các hàm số tuyến tính là các hàm đơn giản nhất, vì chúng thuận tiện trong việc hình dung và tính toán. Chúng ta sẽ được thấy trong các bài viết sau, _tuyến tính_ rất quan trọng và hữu ích trong các bài toán Machine Learning. Kinh nghiệm cá nhân tôi cho thấy, trước khi hiểu được các thuật toán _phi tuyến_ (non-linear, không phẳng), chúng ta cần nắm vững các kỹ thuật cho các mô hình _tuyến tính_.


## 2. Phân tích toán học 

### Dạng của Linear Regression 

Trong phương trình \\((1)\\) phía trên, nếu chúng ta đặt \\(\mathbf{w} = [w_1; w_2; w_3, w_0] = \\) là vector hệ số cần phải tối ưu và \\(\mathbf{\bar{x}} = [x_1; x_2; x_3; 1]\\) là vector dữ liệu đầu vào _mở rộng_. Số \\(1\\) ở cuối được thêm vào để thuận tiện cho việc tính toán. Khi đó, phương trình (1) có thể được viết lại dưới dạng:

\\[y \approx \mathbf{w}^T\mathbf{\bar{x}} = \bar{y}\\]

### Sai số dự đoán 

Chúng ta mong muốn rằng sự sai khác \\(e\\) giữa giá trị thực \\(y\\) và giá trị dự đoán \\(\bar{y}\\) là nhỏ nhất. Nói cách khác, chúng ta muốn giá trị sau đây càng nhỏ càng tốt: 

\\[
\frac{1}{2}e^2 = \frac{1}{2}(y - \bar{y})^2 = \frac{1}{2}(y - \mathbf{w}^T\mathbf{\bar{x}})^2
\\]

trong đó hệ số \\(\frac{1}{2} \\) là để thuận tiện cho việc tính toán (tính đạo hàm mà tôi sẽ trình bày ở phía dưới). Chúng ta cần \\(e^2\\) vì \\(e = y - \bar{y} \\) có thể là một số âm, việc nói \\(e\\) nhỏ nhất sẽ không đúng vì khi \\(e = - \infty\\) là rất nhỏ nhưng sự sai lệch là rất lớn. Bạn đọc có thể tự đặt câu hỏi: **tại sao không dùng trị tuyệt đối \\( \|e\| \\) mà lại dùng bình phương \\(e^2\\) ở đây?** Câu trả lời sẽ có ở phần sau. 


### Hàm mất mát

Điều tương tự xảy ra với tất cả các cặp _(input, outcome)_ \\( (\mathbf{x}_i, y_i), i = 1, 2, \dots, N \\), với \\(N\\) là số lượng dữ liệu quan sát được. Điều chúng ta muốn, tổng sai số là nhỏ nhất, tương đương với việc tìm \\( \mathbf{w} \\) để hàm số sau đạt giá trị nhỏ nhất:

\\[ \mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^N (y_i - \mathbf{w}^T\mathbf{\bar{x}}_i)^2 ~~~~~(2) \\] 

Trong đó \\(\mathbf{w}^T\\) là chuyển vị (transpose) của vector \\(\mathbf{w}\\).
Hàm số \\(\mathcal{L}(\mathbf{w}) \\) được gọi là __hàm mất mát__ (loss function) của bài toán Linear Regression. Chúng ta luôn mong muốn rằng sự mất mát càng nhỏ càng tốt, điều đó đồng nghĩa với việc giá trị của hàm mất mát này càng nhỏ càng tốt. Giá trị của \\(\mathbf{w}\\) làm cho hàm mất mát đạt giá trị nhỏ nhất được gọi là _điểm tối ưu_ (optimal point), ký hiệu:

\\[ \mathbf{w}^* = \arg\min_{\mathbf{w}} \mathcal{L}(\mathbf{w})  \\] 

Trước khi đi tìm lời giải, chúng ta tối giản phép toán trong phương trình hàm mất mát \\((2)\\). Đặt \\(\mathbf{y} = [y_1, y_2, \dots, y_N]\\) là một vector hàng chứa tất cả các _output_ của _training data_; \\( \mathbf{\bar{X}} = [\mathbf{\bar{x}}_1, \mathbf{\bar{x}}_2, \dots, \mathbf{\bar{x}}_N ] \\) là ma trận dữ liệu đầu vào (mở rộng). Khi đó hàm số mất mát \\(\mathcal{L}(\mathbf{w})\\) được viết dưới dạng ma trận đơn giản hơn: 

\\[
\mathcal{L}(\mathbf{w}) = \frac{1}{2}\sum_{i=1}^N (y_i - \mathbf{w}^T\mathbf{\bar{x}}_i)^2 = \frac{1}{2} \\|\mathbf{y} - \mathbf{w}^T \mathbf{\bar{X}}\\|_2^2~~~(3)
\\]

với \\( \\| \mathbf{z} \\|_2 \\) là Euclidean norm (chuẩn Euclid, hay khoảng cách Euclid) và \\( \\| \mathbf{z} \\|_2^2 \\) là tổng của bình phương mỗi phần tử của vector \\(\mathbf{z}\\). Tới đây, ta đã có một dạng đơn giản của hàm mất mát được viết như phương trình \\((3)\\).

### Nghiệm cho bài toán Linear Regression
__Cách phổ biến nhất để tìm nghiệm cho một bài toán tối ưu (chúng ta đã biết từ khi học cấp 3) là giải phương trình đạo hàm bằng 0!__ Tất nhiên đó là khi việc tính đạo hàm và việc giải phương trình đạo hàm bằng 0 không quá phức tạp. Thật may mắn, với các mô hình tuyến tính, hai việc này là khả thi. 

<a name="tiep-vu"></a>

_Đến đây tôi xin quay lại câu hỏi ở phần [Sai số dự đoán](#sai-số-dự-đoán) phía trên về việc tại sao không dùng trị tuyệt đối mà lại dùng bình phương. Câu trả lời là hàm bình phương có đạo hàm tại mọi nơi, hàm trị tuyệt đối thì không (đạo hàm không xác định tại 0)_


## 3. Triển khai trên trên Python

## 4. Thảo luận

### Output là một vector nhiều biến

\\(f(\mathbf{x})\\) là một đa thức bậc cao. 

### Mô hình là một đa thức bậc cao

### Hạn chế của Linear Regression
* Nhạy cảm với nhiễu 
* Không biểu diễn được các mô hình phức tạp 

### Các phương pháp tối ưu

## Tiep Vu

<!-- Giả sử chúng ta có các cặp (_input, outcome_) \\( (\mathbf{x}_1, \mathbf{y}_1), \dots, (\mathbf{x}_N, \mathbf{y}_N) \\), chúng ta phải tìm một hàm  -->
## 5. Tài liệu tham khảo

[Data](http://people.sc.fsu.edu/~jburkardt/datasets/regression/regression.html)