---
layout: post
comments: true
title:  "Support Vector Machine"
title2:  "Support Vector Machine"
date:   2017-06-30 17:22:00
permalink: /svm/
mathjax: true
tags: Machine-Learning SVM Supervised-Learning
categories: Machine-Learning
img: /blog/assets/svm/svm.png
summary: Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane...
---


"Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane."

## 1. Model
Suppose some given data points each belong to one of two classes, there would be many hyperplanes that might classify separate the data into two classes. One logical choice of hyperplane is the one that represents the largest separation between the two classes.
* Distance from a data point \\(\mathbf{x}\\) to a hyperplane \\(\mathbf{w}^T\mathbf{x} + b = 0\\) is:
\\[
\frac{\| \mathbf{w}^T\mathbf{x}\_n + b\|}{\|\|\mathbf{w}\|\|_2}
\\]

where \\(\|\|\mathbf{w}\|\|\_2 = \sqrt{\sum_{i=1}^k w_i^2}\\) and \\(k\\) is number of dimension.
* Support Vectors are data points with the nearest distance to the hyperplane.
* The nearest distance is called margin.

<div class="imgcap">
<div >
    <img src="/blog/assets/svm/svm.png" width = "300">
</div>
</div>

## 2. Estimation
To find the hyperplane that represents the largest separation between the two classes, we can formulate the problem as finding a hyperplane that maximizes the margin,i.e. the nearest distance from data points ti the hyperplane. Suppose the labels for two classes are \\((y = 1)\\) for points above the hyperplane and \\((y = -1)\\) for points below the hyperlane, the distance from a data point \\((\mathbf{x}\_n, y_n)\\) to the hyperplane \\(\mathbf{w}^T\mathbf{x} + b = 0\\) would be:
\\[
\frac{y_n(\mathbf{w}^T\mathbf{x}\_n + b)}{\|\|\mathbf{w}\|\|_2}
\\]

Maximizing margin:
\\[
\arg\max_{\mathbf{w}, b} \left\\{
    \min_{n} \frac{y\_n(\mathbf{w}^T\mathbf{x}\_n + b)}{\|\|\mathbf{w}\|\|\_2} 
\right\\}
= \arg\max_{\mathbf{w}, b}\left\\{
    \frac{1}{\|\|\mathbf{w}\|\|\_2} \min_{n} y\_n(\mathbf{w}^T\mathbf{x}\_n + b)
\right\\}
\\]

Since changing both \\(\mathbf{w}\\) and \\(b\\) by a factor of k>0 result in the the same distance, we could choose a k so that points nearest to the hyperplane would have the below constraint:
\\[
y\_n(\mathbf{w}^T\mathbf{x}\_n + b) = 1
\\]
The problem now becomes:
\\[
\arg \max_{\mathbf{w}, b} \left\\{ \frac{1}{\|\|\mathbf{w}\|\|_2} \right\\} \\\
~subject ~to ~y\_n(\mathbf{w}^T\mathbf{x}\_n + b) \geq 1, \forall n = 1, 2, \dots, N
\\]
Lagrange multiplier method can be used to solve this problem.

## 3. Soft Margin SVM
So far, we assume the data points are linearly seperatable and the technique described above is called Hard Margin SVM. In cases where data points are almost linearly seperatable, we can relax the constraint by introducing a slack variable:
\\[
y\_n(\mathbf{w}^T\mathbf{x}\_n + b) \geq 1-\xi_n, \forall n = 1, 2, \dots, N
\\]
* \\(\xi_n\\) = 0: data point is on margin or correct side of margin
* 0 < \\(\xi_n\\) < 1: data point is inside margin but is still considered correct classification
* \\(\xi_n\\) > 1: data point is misclassified
We want to minimize non-zero slack variables while maximizing the margin. The objective would now be stated as:
\\[
\arg \min_{\mathbf{w}, b, \xi} \left\\{ \frac{1}{2}{\|\|\mathbf{w}\|\|\_2^2} + C \sum_{n=1}^N \xi\_n \right\\} \\\
~subject ~to ~y\_n(\mathbf{w}^T\mathbf{x}\_n + b) \geq 1-\xi_n, ~and ~\xi_n \geq 0 ~\forall n = 1, 2, \dots, N
\\]
or
\\[
\arg \min_{\mathbf{w}, b, \xi} \left\\{ \frac{1}{2}{\|\|\mathbf{w}\|\|\_2^2} + C \sum_{n=1}^N \xi\_n \right\\} \\\
~subject ~to ~\xi_n \geq \max(0, 1 - y\_n(\mathbf{w}^T\mathbf{x}\_n + b)) ~\forall n = 1, 2, \dots, N
\\]
in which hyperparameter C > 0 controls importance of large margin versus incorrect classification (non-zero slack).

Since \\(\xi_n > 0, \forall n = 1, 2, \dots, N\\), the solution must satisfy:
\\[
~\xi_n = \max(0, 1 - y\_n(\mathbf{w}^T\mathbf{x}\_n + b)) ~\forall n = 1, 2, \dots, N
\\]
Therefore, the problem becomes:
\\[
\arg \min_{\mathbf{w}, b} \left\\{ \frac{1}{2}{\|\|\mathbf{w}\|\|\_2^2} + C \sum_{n=1}^N \max(0, 1 - y_n(\mathbf{w}^T\mathbf{x}\_n + b)) \right\\}
\\]
\\(E_n(\mathbf{w}, b) = \max(0, 1 - y_n(\mathbf{w}^T\mathbf{x}\_n + b))\\) is called hinge loss, another common loss function used in classification besides cross entropy loss. Hinge loss does not care about the details of the individual loss as long as it is less than 0, i,e it only punishes misclassification. On the other hand, cross entropy loss is never fully happy with any individual loss and punish every point depending on how far the prediction from 0 or 1.

We can use gradient descent to perform parameter estimation.

## 4. Multiclass SVM
We can extend Support Vector Machine to handle multi-class cases by accumulating loss unless the score of the correct class y is larger than the incorrect class scores by at least by \\(\Delta\\), a hyperparameter can be set using cross validation.
\\[
E(y,\mathbf{x},\mathbf{w}) = \sum_{j \neq y} \max(0, \Delta - \mathbf{w}\_y^T\mathbf{x} + \mathbf{w}\_j^T\mathbf{x})
\\]
The loss function over N data points would be:
\\[
E(y,\mathbf{x},\mathbf{w}) = \frac{1}{N}\sum_{n=1}^N \sum\_{j \neq y_n} \max(0, \Delta - \mathbf{w}\_y^T\mathbf{x} + \mathbf{w}\_j^T\mathbf{x})
\\]

## 5. Kernel SVM
In cases where data points are totally linearly seperatable, we need to transform the feature space into a higher dimensional space in order to find linearly seperatable hyperplane, as illustrated below:
<div class="imgcap">
<div >
    <img src="/blog/assets/svm/kernel.png" width = "700">
</div>
</div>

For such feature space transformation, we usually apply a kernel function, which can make use of kernel trick (i.e. simply computing the inner products between the all pairs of data in the feature space without ever computing the coordinates of the data in that space) in order to avoid expensive computation. Below are commonly used kernel functions:
* Polynomial:
\\[
k(\mathbf{x}, \mathbf{z}) = (r + \gamma \mathbf{x}^T\mathbf{z})^d
\\]
* Sigmoid:
\\[
k(\mathbf{x}, \mathbf{z}) = \exp(-\gamma \|\|\mathbf{x} - \mathbf{z}\|\|_2^2), ~~ \gamma > 0
\\]
* Radial Basic Function:
\\[
k(\mathbf{x}, \mathbf{z}) = \exp(-\gamma \|\|\mathbf{x} - \mathbf{z}\|\|_2^2), ~~ \gamma > 0
\\]
