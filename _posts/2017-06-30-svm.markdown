---
layout: post
comments: true
title:  "Support Vector Machine"
title2:  "Support Vector Machine"
date:   2017-06-30 17:22:00
permalink: /svm/
mathjax: true
tags: Machine-Learning SVM Supervised-Learning
categories: Machine-Learning
img: /blog/assets/svm/svm.png
summary: Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane...
---


"Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane."

## 1. Model
Suppose some given data points each belong to one of two classes, there would be many hyperplanes that might classify separate the data into two classes. One logical choice of hyperplane is the one that represents the largest separation between the two classes.
* Distance from a data point \\(\mathbf{x}\\) to a hyperplane \\(\mathbf{w}^T\mathbf{x} + b = 0\\) is:
\\[
\frac{\| \mathbf{w}^T\mathbf{x}\_n + b\|}{\|\|\mathbf{w}\|\|_2}
\\]

where \\(\|\|\mathbf{w}\|\|\_2 = \sqrt{\sum_{i=1}^k w_i^2}\\) and \\(k\\) is number of dimension.
* Support Vectors are data points with the nearest distance to the hyperplane.
* The nearest distance is called margin.

<div class="imgcap">
<div >
    <img src="/blog/assets/svm/svm.png" width = "300">
</div>
</div>

## 2. Estimation
To find the hyperplane that represents the largest separation between the two classes, we can formulate the problem as finding a hyperplane that maximizes the margin,i.e. the nearest distance from data points ti the hyperplane. Suppose the labels for two classes are \\((y = 1)\\) for points above the hyperplane and \\((y = -1)\\) for points below the hyperlane, the distance from a data point \\((\mathbf{x}\_n, y_n)\\) to the hyperplane \\(\mathbf{w}^T\mathbf{x} + b = 0\\) would be:
\\[
\frac{y_n(\mathbf{w}^T\mathbf{x}\_n + b)}{\|\|\mathbf{w}\|\|_2}
\\]

Maximizing margin:
\\[
\arg\max_{\mathbf{w}, b} \left\\{
    \min_{n} \frac{y\_n(\mathbf{w}^T\mathbf{x}\_n + b)}{\|\|\mathbf{w}\|\|\_2} 
\right\\}
= \arg\max_{\mathbf{w}, b}\left\\{
    \frac{1}{\|\|\mathbf{w}\|\|\_2} \min_{n} y\_n(\mathbf{w}^T\mathbf{x}\_n + b)
\right\\}
\\]

Since changing both \\(\mathbf{w}\\) and \\(b\\) by a factor of k>0 result in the the same distance, we could choose a k so that points nearest to the hyperplane would have the below constraint:
\\[
y\_n(\mathbf{w}^T\mathbf{x}\_n + b) = 1
\\]
The problem now becomes:
\\[
\arg \max_{\mathbf{w}, b} \frac{1}{\|\|\mathbf{w}\|\|_2} ~or ~\arg \min_{\mathbf{w}, b} \|\|\mathbf{w}\|\|_2 \\\
~subject ~to ~y\_n(\mathbf{w}^T\mathbf{x}\_n + b) \geq 1, \forall n = 1, 2, \dots, N
\\]
Lagrange multiplier method can be used to solve this problem.

## 3. Soft Margin SVM
So far, we assume the data points are linearly seperatable and the technique described above is called Hard Margin SVM. In cases where data points are not linearly seperatable, we can relax the constraint by introducing a slack variable:
\\[
y\_n(\mathbf{w}^T\mathbf{x}\_n + b) \geq 1-\xi_n, \forall n = 1, 2, \dots, N
\\]
* \\(\xi_n\\) = 0: data point is on margin or correct side of margin
* 0 < \\(\xi_n\\) < 1: data point is inside margin but is still considered correct classification
* \\(\xi_n\\) > 1: data point is misclassified
We want to minimize non-zero slack variables while maximizing the margin. The problem becomes:
\\[
\arg \min_{\mathbf{w}, b} \|\|\mathbf{w}\|\|_2 + \\\
~subject ~to ~y\_n(\mathbf{w}^T\mathbf{x}\_n + b) \geq 1, \forall n = 1, 2, \dots, N
\\]
